Corrected the Comprehend Lambda today and got it working but to my dismay it was taking too much time to convert texts to keywords and the output the Comprehend API stored in S3 is even more of a problem. It  creating a tar directory inside which is another tar directory, inside which is the json file. Also the roles needed to run this thing using lambdas are were quite confusing to me, first we have the permission for lambda to call comprehend  apis adding which is quite simple, but the problem came when in the [request](https://docs.aws.amazon.com/comprehend/latest/dg/API_StartKeyPhrasesDetectionJob.html) for the Comprehend API I had to provide the DataRoleAccessARN.

This taught me a lot regarding different types of policy like permissions and trust policies. A role is determined by both permissions and trust policies. Trust policies define whether the service/user this role is assigned to is allowed to be using this role. The permissions determine what all services can be accessed if this role is being used.
I had S3Crud Role.

with permission as
- AmazonS3FullAccess
- CloudWatchLogsFullAccess

Which basically this role allows access of all S3 services and all cloudwatch services.

While the trust policies were given as follows:
```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    },
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "comprehend.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
```

This meant that this role could be added to lambdas and comprehend apis.

This is what had to be filled inside the request field DataAccessRoleArn in the comprehend API, thus providing access to the S3 and having Comprehend in the trust policy.


Finally, I ended up not choosing comprehend because of its slow speed (some error on my side probably) and its unhelpful output format. 

What I have decided to do is create another lambda which reads the transcription file and gets the text and uses the library keyword extractor to extract basic keywords from the text, after that I could do some manual pruning to the data and pass the data to elastic search all in a single lambda function, thus reducing complexity a lot.

Key word extractor basically uses english stop word list and removes all these words from the text and throws out the remaining words. An effecient implementation for this could be done using tries but currently I would be doing this with the library and after completing the project and maybe provide some optimization.


- Decided to drop Elastic Search solution for indexing and use traditional and well documented Full text search for Postgres. Went with postgres as 